{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "30l9ZyTjxJjf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import datagen # prevent circular module import exception\n",
    "from transformers import AutoTokenizer, AutoConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from modelling.collator import BetterDataCollatorForWholeWordMask\n",
    "from modelling.model import SynsetClassificationModel\n",
    "from modelling.trainer import BetterTrainer\n",
    "from lit_nlp.api import types as lit_types\n",
    "from lit_nlp.api.dataset import Dataset\n",
    "from lit_nlp.api.model import Model\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str):\n",
    "    ds = datagen.SemCorDataSet.unpickle(Path(path).with_suffix(\".pickle\"))\n",
    "    hf_ds = datasets.Dataset.load_from_disk(Path(path).with_suffix(\".hf\"))\n",
    "    \n",
    "    hf_ds = hf_ds.add_column(\"sense-labels\", hf_ds[\"labels\"])\n",
    "    relevant_columns = [\n",
    "        column\n",
    "        for column in hf_ds.column_names\n",
    "        if column not in ds.sentence_level.columns\n",
    "    ]\n",
    "    relevant_columns.append(\"sense-labels\")\n",
    "    hf_ds.set_format(type=\"torch\", columns=relevant_columns)\n",
    "    \n",
    "    return hf_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path: str):\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        print(f\"CUDA found; running on {device}\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(f\"CUDA not found; running on {device}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(path, local_files_only=True)\n",
    "    \n",
    "    model = SynsetClassificationModel.from_pretrained(\n",
    "        path,\n",
    "        config=AutoConfig.from_pretrained(path, local_files_only=True),\n",
    "        local_files_only=True,\n",
    "        model_name=path,\n",
    "        num_classes=2584,\n",
    "    ).to(device)\n",
    "    \n",
    "    trainer = BetterTrainer(\n",
    "        model=model,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer),\n",
    "        args=TrainingArguments(\n",
    "            output_dir='./saliency',\n",
    "            remove_unused_columns=False,\n",
    "            label_names=[\"labels\", \"sense-labels\"]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to https://pair-code.github.io/lit/setup/ for implementation details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIT_Dataset(Dataset):\n",
    "    def __init__(self, path: str):\n",
    "        ds = load_dataset(path)\n",
    "        \n",
    "        self._examples = [{\n",
    "            #'sentence_idx': row['sentence_idx'],\n",
    "            #'sentence': row['sentence'],\n",
    "            'input_ids': row['input_ids'],\n",
    "            'attention_mask': row['attention_mask'].tolist(),\n",
    "            'labels': row['labels'].tolist(),\n",
    "            'sense-labels': row['sense-labels'].tolist(),\n",
    "        } for row in ds]\n",
    "        \n",
    "        #self._examples = ds\n",
    "        \n",
    "\n",
    "    def spec(self):\n",
    "        return {\n",
    "            #'sentence_idx': lit_types.Scalar(),\n",
    "            #'sentence': lit_types.TextSegment(),\n",
    "            'input_ids': lit_types.TokenEmbeddings(),\n",
    "            'attention_mask': lit_types.TokenEmbeddings(),\n",
    "            'labels': lit_types.TokenEmbeddings(),\n",
    "            'sense-labels': lit_types.TokenEmbeddings(),\n",
    "        }\n",
    "\n",
    "class LIT_Model(Model):\n",
    "    def __init__(self, path: str):\n",
    "        self._model = load_model(path)\n",
    "        \n",
    "    def input_spec(self):\n",
    "        return {\n",
    "            'input_ids': lit_types.TokenEmbeddings(),\n",
    "            'attention_mask': lit_types.TokenEmbeddings(),\n",
    "            'sense-labels': lit_types.TokenEmbeddings(),\n",
    "        }\n",
    "    \n",
    "    def output_spec(self):\n",
    "        return {\n",
    "            'labels': lit_types.TokenEmbeddings(),\n",
    "        }\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        return self._model.predict(inputs)\n",
    "    \n",
    "    def predict_minibatch(self, inputs):\n",
    "        return self._model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "AWhbAZg57RpB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./checkpoints-probing/roberta-probing+semcor/checkpoint-185900/added_tokens.json. We won't load it.\n",
      "loading file ./checkpoints-probing/roberta-probing+semcor/checkpoint-185900/vocab.json\n",
      "loading file ./checkpoints-probing/roberta-probing+semcor/checkpoint-185900/merges.txt\n",
      "loading file ./checkpoints-probing/roberta-probing+semcor/checkpoint-185900/tokenizer.json\n",
      "loading file None\n",
      "loading file ./checkpoints-probing/roberta-probing+semcor/checkpoint-185900/special_tokens_map.json\n",
      "loading file ./checkpoints-probing/roberta-probing+semcor/checkpoint-185900/tokenizer_config.json\n",
      "loading configuration file ./checkpoints-probing/roberta-probing+semcor/checkpoint-185900/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./checkpoints-probing/roberta-probing+semcor/checkpoint-185900\",\n",
      "  \"architectures\": [\n",
      "    \"SynsetClassificationModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./checkpoints-probing/roberta-probing+semcor/checkpoint-185900/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not found; running on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./checkpoints-probing/roberta-probing+semcor/checkpoint-185900/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./checkpoints-probing/roberta-probing+semcor/checkpoint-185900\",\n",
      "  \"architectures\": [\n",
      "    \"SynsetClassificationModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./checkpoints-probing/roberta-probing+semcor/checkpoint-185900/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ./checkpoints-probing/roberta-probing+semcor/checkpoint-185900 were not used when initializing RobertaModel: ['mlmodel.encoder.layer.1.attention.output.LayerNorm.weight', 'mlmodel.encoder.layer.3.intermediate.dense.bias', 'mlmodel.encoder.layer.4.attention.output.dense.weight', 'mlmodel.encoder.layer.5.attention.output.dense.weight', 'mlmodel.encoder.layer.8.attention.output.dense.weight', 'mlmodel.encoder.layer.10.intermediate.dense.weight', 'mlmodel.encoder.layer.10.output.dense.weight', 'mlmodel.encoder.layer.4.attention.output.LayerNorm.bias', 'mlmodel.encoder.layer.9.intermediate.dense.weight', 'mlmodel.encoder.layer.1.output.LayerNorm.weight', 'mlmodel.encoder.layer.11.output.LayerNorm.weight', 'mlmodel.encoder.layer.5.attention.output.dense.bias', 'mlmodel.encoder.layer.2.output.LayerNorm.weight', 'mlmodel.encoder.layer.1.attention.self.value.bias', 'mlmodel.encoder.layer.9.attention.self.key.bias', 'mlmodel.encoder.layer.10.attention.self.value.bias', 'mlmodel.encoder.layer.3.attention.output.dense.weight', 'mlmodel.encoder.layer.1.attention.self.key.bias', 'mlmodel.encoder.layer.2.attention.output.LayerNorm.bias', 'mlmodel.encoder.layer.11.attention.self.key.bias', 'mlmodel.encoder.layer.5.attention.self.query.weight', 'mlmodel.encoder.layer.2.attention.output.LayerNorm.weight', 'mlmodel.encoder.layer.0.output.LayerNorm.bias', 'mlmodel.encoder.layer.11.attention.self.value.weight', 'mlmodel.embeddings.position_ids', 'mlmodel.encoder.layer.4.output.LayerNorm.weight', 'mlmodel.encoder.layer.6.attention.self.query.weight', 'mlmodel.encoder.layer.5.output.dense.weight', 'mlmodel.encoder.layer.5.output.LayerNorm.weight', 'mlmodel.encoder.layer.1.attention.self.query.bias', 'mlmodel.encoder.layer.10.output.dense.bias', 'mlmodel.encoder.layer.3.attention.self.value.bias', 'mlmodel.encoder.layer.0.attention.self.key.bias', 'mlmodel.encoder.layer.9.output.LayerNorm.bias', 'mlmodel.encoder.layer.9.attention.self.value.bias', 'mlmodel.encoder.layer.11.attention.self.key.weight', 'mlmodel.encoder.layer.6.attention.output.dense.weight', 'mlmodel.encoder.layer.7.attention.self.key.bias', 'mlmodel.encoder.layer.6.output.dense.weight', 'mlmodel.encoder.layer.10.attention.output.LayerNorm.bias', 'mlmodel.encoder.layer.4.attention.self.key.bias', 'mlmodel.encoder.layer.9.output.dense.weight', 'mlmodel.encoder.layer.5.output.dense.bias', 'mlmodel.encoder.layer.11.output.dense.weight', 'mlmodel.encoder.layer.9.attention.self.query.bias', 'mlmodel.encoder.layer.0.attention.self.value.bias', 'mlmodel.encoder.layer.1.attention.output.dense.bias', 'mlmodel.pooler.dense.bias', 'mlmodel.encoder.layer.3.output.dense.weight', 'mlmodel.encoder.layer.10.intermediate.dense.bias', 'mlmodel.encoder.layer.3.attention.self.query.bias', 'mlmodel.encoder.layer.7.attention.self.value.weight', 'mlmodel.encoder.layer.3.attention.self.key.weight', 'mlmodel.encoder.layer.1.output.dense.weight', 'mlmodel.encoder.layer.10.attention.output.LayerNorm.weight', 'mlmodel.encoder.layer.2.intermediate.dense.bias', 'mlmodel.encoder.layer.1.attention.self.key.weight', 'mlmodel.encoder.layer.4.output.LayerNorm.bias', 'mlmodel.encoder.layer.8.attention.output.LayerNorm.weight', 'mlmodel.encoder.layer.10.attention.output.dense.weight', 'mlmodel.encoder.layer.8.attention.self.query.bias', 'mlmodel.encoder.layer.10.attention.self.query.bias', 'mlmodel.encoder.layer.0.attention.self.value.weight', 'mlmodel.encoder.layer.7.output.LayerNorm.bias', 'mlmodel.encoder.layer.3.attention.self.value.weight', 'classifier.3.weight', 'mlmodel.encoder.layer.5.attention.self.value.weight', 'mlmodel.encoder.layer.4.intermediate.dense.weight', 'mlmodel.encoder.layer.8.attention.self.key.weight', 'mlmodel.encoder.layer.2.output.LayerNorm.bias', 'mlmodel.encoder.layer.8.attention.output.dense.bias', 'mlmodel.encoder.layer.8.intermediate.dense.weight', 'mlmodel.encoder.layer.9.attention.output.dense.bias', 'mlmodel.encoder.layer.5.attention.self.key.weight', 'mlmodel.encoder.layer.8.attention.self.query.weight', 'mlmodel.encoder.layer.7.attention.self.key.weight', 'mlmodel.embeddings.LayerNorm.bias', 'mlmodel.encoder.layer.4.attention.self.value.bias', 'mlmodel.encoder.layer.5.intermediate.dense.bias', 'mlmodel.encoder.layer.4.attention.output.LayerNorm.weight', 'mlmodel.encoder.layer.5.output.LayerNorm.bias', 'mlmodel.encoder.layer.6.attention.self.value.bias', 'mlmodel.encoder.layer.7.attention.output.LayerNorm.bias', 'mlmodel.encoder.layer.11.attention.self.value.bias', 'mlmodel.encoder.layer.0.attention.output.LayerNorm.weight', 'mlmodel.encoder.layer.8.output.dense.bias', 'mlmodel.encoder.layer.10.attention.self.value.weight', 'mlmodel.encoder.layer.5.attention.self.value.bias', 'mlmodel.pooler.dense.weight', 'mlmodel.encoder.layer.6.attention.output.LayerNorm.weight', 'mlmodel.encoder.layer.0.attention.output.LayerNorm.bias', 'mlmodel.encoder.layer.5.attention.output.LayerNorm.weight', 'mlmodel.encoder.layer.11.attention.output.dense.bias', 'mlmodel.encoder.layer.1.intermediate.dense.bias', 'mlmodel.encoder.layer.2.attention.self.value.weight', 'mlmodel.encoder.layer.6.output.LayerNorm.weight', 'mlmodel.encoder.layer.3.output.LayerNorm.weight', 'mlmodel.encoder.layer.2.intermediate.dense.weight', 'mlmodel.embeddings.LayerNorm.weight', 'mlmodel.encoder.layer.0.attention.self.query.bias', 'mlmodel.encoder.layer.4.attention.self.query.weight', 'mlmodel.encoder.layer.5.attention.output.LayerNorm.bias', 'mlmodel.encoder.layer.8.attention.output.LayerNorm.bias', 'mlmodel.encoder.layer.0.attention.output.dense.weight', 'mlmodel.encoder.layer.8.output.dense.weight', 'mlmodel.encoder.layer.9.attention.output.LayerNorm.bias', 'mlmodel.encoder.layer.7.attention.self.value.bias', 'classifier.1.bias', 'mlmodel.encoder.layer.9.output.LayerNorm.weight', 'mlmodel.encoder.layer.3.attention.self.query.weight', 'mlmodel.encoder.layer.4.attention.output.dense.bias', 'mlmodel.encoder.layer.1.attention.self.value.weight', 'mlmodel.encoder.layer.10.output.LayerNorm.weight', 'mlmodel.encoder.layer.0.output.dense.bias', 'mlmodel.encoder.layer.0.output.LayerNorm.weight', 'mlmodel.encoder.layer.7.intermediate.dense.bias', 'mlmodel.encoder.layer.7.output.dense.bias', 'mlmodel.encoder.layer.9.attention.output.LayerNorm.weight', 'mlmodel.encoder.layer.2.attention.self.query.bias', 'mlmodel.encoder.layer.3.output.dense.bias', 'mlmodel.encoder.layer.10.attention.self.key.bias', 'mlmodel.encoder.layer.0.intermediate.dense.weight', 'mlmodel.encoder.layer.3.attention.self.key.bias', 'mlmodel.encoder.layer.8.attention.self.value.weight', 'mlmodel.encoder.layer.0.output.dense.weight', 'mlmodel.encoder.layer.5.intermediate.dense.weight', 'mlmodel.encoder.layer.7.intermediate.dense.weight', 'mlmodel.encoder.layer.11.intermediate.dense.bias', 'mlmodel.encoder.layer.11.output.dense.bias', 'mlmodel.encoder.layer.9.attention.self.key.weight', 'classifier.1.weight', 'mlmodel.encoder.layer.3.attention.output.LayerNorm.bias', 'mlmodel.encoder.layer.9.output.dense.bias', 'mlmodel.encoder.layer.4.attention.self.key.weight', 'mlmodel.encoder.layer.11.attention.self.query.weight', 'mlmodel.encoder.layer.10.attention.self.key.weight', 'mlmodel.encoder.layer.1.attention.output.dense.weight', 'mlmodel.encoder.layer.4.attention.self.value.weight', 'mlmodel.encoder.layer.7.output.LayerNorm.weight', 'mlmodel.encoder.layer.1.output.LayerNorm.bias', 'mlmodel.encoder.layer.6.attention.output.dense.bias', 'mlmodel.encoder.layer.6.output.LayerNorm.bias', 'mlmodel.encoder.layer.7.attention.output.LayerNorm.weight', 'mlmodel.encoder.layer.8.attention.self.value.bias', 'mlmodel.encoder.layer.4.intermediate.dense.bias', 'mlmodel.encoder.layer.7.output.dense.weight', 'mlmodel.encoder.layer.8.output.LayerNorm.weight', 'mlmodel.encoder.layer.9.attention.self.query.weight', 'mlmodel.encoder.layer.6.output.dense.bias', 'mlmodel.encoder.layer.7.attention.output.dense.bias', 'mlmodel.encoder.layer.11.output.LayerNorm.bias', 'mlmodel.encoder.layer.10.output.LayerNorm.bias', 'mlmodel.encoder.layer.4.attention.self.query.bias', 'mlmodel.encoder.layer.6.attention.output.LayerNorm.bias', 'mlmodel.encoder.layer.3.output.LayerNorm.bias', 'mlmodel.encoder.layer.5.attention.self.key.bias', 'mlmodel.encoder.layer.11.attention.self.query.bias', 'mlmodel.encoder.layer.0.intermediate.dense.bias', 'mlmodel.encoder.layer.1.attention.self.query.weight', 'mlmodel.encoder.layer.10.attention.self.query.weight', 'mlmodel.encoder.layer.1.attention.output.LayerNorm.bias', 'mlmodel.encoder.layer.7.attention.self.query.weight', 'mlmodel.encoder.layer.6.attention.self.query.bias', 'mlmodel.encoder.layer.0.attention.output.dense.bias', 'mlmodel.encoder.layer.4.output.dense.bias', 'classifier.3.bias', 'mlmodel.encoder.layer.1.output.dense.bias', 'mlmodel.encoder.layer.2.attention.self.key.weight', 'mlmodel.encoder.layer.2.output.dense.bias', 'mlmodel.encoder.layer.2.attention.self.query.weight', 'mlmodel.encoder.layer.11.attention.output.LayerNorm.bias', 'mlmodel.encoder.layer.2.attention.output.dense.weight', 'mlmodel.encoder.layer.2.output.dense.weight', 'mlmodel.embeddings.position_embeddings.weight', 'mlmodel.encoder.layer.0.attention.self.key.weight', 'mlmodel.encoder.layer.6.attention.self.value.weight', 'mlmodel.encoder.layer.9.intermediate.dense.bias', 'mlmodel.encoder.layer.11.attention.output.LayerNorm.weight', 'mlmodel.encoder.layer.6.intermediate.dense.bias', 'mlmodel.encoder.layer.3.intermediate.dense.weight', 'mlmodel.encoder.layer.1.intermediate.dense.weight', 'mlmodel.encoder.layer.10.attention.output.dense.bias', 'mlmodel.encoder.layer.2.attention.self.key.bias', 'mlmodel.encoder.layer.2.attention.output.dense.bias', 'mlmodel.encoder.layer.7.attention.self.query.bias', 'mlmodel.encoder.layer.9.attention.self.value.weight', 'mlmodel.encoder.layer.8.intermediate.dense.bias', 'mlmodel.encoder.layer.11.intermediate.dense.weight', 'mlmodel.encoder.layer.6.attention.self.key.bias', 'mlmodel.encoder.layer.6.attention.self.key.weight', 'mlmodel.encoder.layer.7.attention.output.dense.weight', 'mlmodel.encoder.layer.8.attention.self.key.bias', 'mlmodel.encoder.layer.11.attention.output.dense.weight', 'mlmodel.encoder.layer.2.attention.self.value.bias', 'mlmodel.encoder.layer.3.attention.output.dense.bias', 'mlmodel.encoder.layer.6.intermediate.dense.weight', 'mlmodel.embeddings.token_type_embeddings.weight', 'mlmodel.encoder.layer.5.attention.self.query.bias', 'mlmodel.embeddings.word_embeddings.weight', 'mlmodel.encoder.layer.4.output.dense.weight', 'mlmodel.encoder.layer.9.attention.output.dense.weight', 'mlmodel.encoder.layer.8.output.LayerNorm.bias', 'mlmodel.encoder.layer.0.attention.self.query.weight', 'mlmodel.encoder.layer.3.attention.output.LayerNorm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./checkpoints-probing/roberta-probing+semcor/checkpoint-185900 and are newly initialized: ['encoder.layer.5.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'pooler.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'pooler.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing SynsetClassificationModel.\n",
      "\n",
      "All the weights of SynsetClassificationModel were initialized from the model checkpoint at ./checkpoints-probing/roberta-probing+semcor/checkpoint-185900.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use SynsetClassificationModel for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tensor([    0,   133,  1808,     9,   464,    12,  4506,   154,    16, 28178,\n            7,     5,  2370,  2156,     8,  2156,   101,   144,  2370, 28178,\n         2192,  2156, 45467, 37448,  4748,     7,     5,  1079,     9,     5,\n          232,   479,     2,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1]) is not JSON serializable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27125/776122793.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlit_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'wsd'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLIT_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./dataset/roberta+senseval2.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mwidget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLitWidget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/bert-wsd-pBoE7GPv-py3.8/lib/python3.8/site-packages/lit_nlp/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, height, render, proxy_url, layouts, *args, **kw)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mlit_demo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayouts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mapp_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_server\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwsgi_serving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotebookWsgiServer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit_demo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxy_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxy_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/bert-wsd-pBoE7GPv-py3.8/lib/python3.8/site-packages/lit_nlp/dev_server.py\u001b[0m in \u001b[0;36mserve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_lit_logo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting LIT server...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m       \u001b[0mapp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlit_app\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLitApp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_app_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_app_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0;31m# If using a separate server program to serve the app, such as gunicorn,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/bert-wsd-pBoE7GPv-py3.8/lib/python3.8/site-packages/lit_nlp/app.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, models, datasets, generators, interpreters, annotators, layouts, data_dir, warm_start, warm_projections, client_root, demo_mode, default_layout, canonical_url, page_title, development_demo)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;31m# Index all datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m     self._datasets = lit_dataset.IndexedDataset.index_all(\n\u001b[0m\u001b[1;32m    440\u001b[0m         self._datasets, caching.input_hash)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/bert-wsd-pBoE7GPv-py3.8/lib/python3.8/site-packages/lit_nlp/api/dataset.py\u001b[0m in \u001b[0;36mindex_all\u001b[0;34m(cls, datasets, id_fn)\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mindex_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIdFnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;34m\"\"\"Convenience function to convert a dict of datasets.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_fn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/bert-wsd-pBoE7GPv-py3.8/lib/python3.8/site-packages/lit_nlp/api/dataset.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mindex_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIdFnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;34m\"\"\"Convenience function to convert a dict of datasets.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_fn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/bert-wsd-pBoE7GPv-py3.8/lib/python3.8/site-packages/lit_nlp/api/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, id_fn, indexed_examples, *args, **kw)\u001b[0m\n\u001b[1;32m    217\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexed_examples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indexed_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indexed_examples\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/bert-wsd-pBoE7GPv-py3.8/lib/python3.8/site-packages/lit_nlp/api/dataset.py\u001b[0m in \u001b[0;36mindex_inputs\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    200\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mindex_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIndexedInput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;34m\"\"\"Create indexed versions of inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     return [\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mIndexedInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'meta'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/bert-wsd-pBoE7GPv-py3.8/lib/python3.8/site-packages/lit_nlp/api/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;34m\"\"\"Create indexed versions of inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     return [\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mIndexedInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'meta'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     ]  # pyformat: disable\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/bert-wsd-pBoE7GPv-py3.8/lib/python3.8/site-packages/lit_nlp/lib/caching.py\u001b[0m in \u001b[0;36minput_hash\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minput_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;34m\"\"\"Create stable hash of an input example.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   json_str = serialize.to_json(\n\u001b[0m\u001b[1;32m     43\u001b[0m       example, simple=True, sort_keys=True).encode(\"utf-8\")\n\u001b[1;32m     44\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/bert-wsd-pBoE7GPv-py3.8/lib/python3.8/site-packages/lit_nlp/lib/serialize.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(obj, simple, **json_kw)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mjson_kw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;34m\"\"\"Serialize to a JSON string.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m   return json.dumps(\n\u001b[0m\u001b[1;32m    113\u001b[0m       obj, cls=SimpleJSONEncoder if simple else CustomJSONEncoder, **json_kw)\n",
      "\u001b[0;32m/usr/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     return cls(\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mskipkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ascii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/bert-wsd-pBoE7GPv-py3.8/lib/python3.8/site-packages/lit_nlp/lib/serialize.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_obj_to_json_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/bert-wsd-pBoE7GPv-py3.8/lib/python3.8/site-packages/lit_nlp/lib/serialize.py\u001b[0m in \u001b[0;36m_obj_to_json_simple\u001b[0;34m(o)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is not JSON serializable.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tensor([    0,   133,  1808,     9,   464,    12,  4506,   154,    16, 28178,\n            7,     5,  2370,  2156,     8,  2156,   101,   144,  2370, 28178,\n         2192,  2156, 45467, 37448,  4748,     7,     5,  1079,     9,     5,\n          232,   479,     2,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1]) is not JSON serializable."
     ]
    }
   ],
   "source": [
    "# Create the LIT widget with the model and dataset to analyze.\n",
    "from lit_nlp import notebook\n",
    "\n",
    "#datasets = {'sst_dev': glue.SST2Data('validation')}\n",
    "lit_models = {'wsd': LIT_Model('./checkpoints-probing/roberta-probing+semcor/checkpoint-185900')}\n",
    "lit_datasets = {'wsd': LIT_Dataset('./dataset/roberta+senseval2.pickle')}\n",
    "\n",
    "widget = notebook.LitWidget(lit_models, lit_datasets, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GSfs1waBdLd"
   },
   "outputs": [],
   "source": [
    "# Render the widget\n",
    "widget.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('./dataset/roberta+senseval2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]['attention_mask'].tolist()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LIT in Notebooks",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
