{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30l9ZyTjxJjf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import datagen # prevent circular module import exception\n",
    "from transformers import AutoTokenizer, AutoConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from modelling.collator import BetterDataCollatorForWholeWordMask\n",
    "from modelling.model import SynsetClassificationModel\n",
    "from modelling.trainer import BetterTrainer\n",
    "from lit_nlp.api import types as lit_types\n",
    "from lit_nlp.api.dataset import Dataset\n",
    "from lit_nlp.api.model import Model\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str):\n",
    "    ds = datagen.SemCorDataSet.unpickle(Path(path).with_suffix(\".pickle\"))\n",
    "    hf_ds = datasets.Dataset.load_from_disk(Path(path).with_suffix(\".hf\"))\n",
    "    \n",
    "    hf_ds = hf_ds.add_column(\"sense-labels\", hf_ds[\"labels\"])\n",
    "    relevant_columns = [\n",
    "        column\n",
    "        for column in hf_ds.column_names\n",
    "        if column not in ds.sentence_level.columns\n",
    "    ]\n",
    "    relevant_columns.append(\"sense-labels\")\n",
    "    hf_ds.set_format(type=\"torch\", columns=relevant_columns)\n",
    "    \n",
    "    return hf_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path: str):\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        print(f\"CUDA found; running on {device}\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(f\"CUDA not found; running on {device}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(path, local_files_only=True)\n",
    "    \n",
    "    model = SynsetClassificationModel.from_pretrained(\n",
    "        path,\n",
    "        config=AutoConfig.from_pretrained(path, local_files_only=True),\n",
    "        local_files_only=True,\n",
    "        model_name=path,\n",
    "        num_classes=2584,\n",
    "    ).to(device)\n",
    "    \n",
    "    trainer = BetterTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer),\n",
    "        args=TrainingArguments(\n",
    "            output_dir='./saliency',\n",
    "            remove_unused_columns=False,\n",
    "            label_names=[\"labels\", \"sense-labels\"]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to https://pair-code.github.io/lit/setup/ for implementation details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIT_Dataset(Dataset):\n",
    "    def __init__(self, path: str):\n",
    "        ds = load_dataset(path)\n",
    "        \n",
    "        self._examples = [{\n",
    "            #'sentence_idx': row['sentence_idx'],\n",
    "            #'sentence': row['sentence'],\n",
    "            'input_ids': row['input_ids'].tolist(),\n",
    "            'attention_mask': row['attention_mask'].tolist(),\n",
    "            'labels': row['labels'].tolist(),\n",
    "            'sense-labels': row['sense-labels'].tolist(),\n",
    "        } for row in ds]\n",
    "        \n",
    "        # adding sentence will crash (can't create tensor, try padding, whatever)\n",
    "        #for i in range(len(self._examples)):\n",
    "            #self._examples[i]['sentence'] = ds['sentence'][i]\n",
    "        \n",
    "    # probably wrong types?!\n",
    "    def spec(self):\n",
    "        return {\n",
    "            #'sentence_idx': lit_types.Scalar(),\n",
    "            #'sentence': lit_types.TextSegment(),\n",
    "            'input_ids': lit_types.Embeddings(),\n",
    "            'attention_mask': lit_types.Embeddings(),\n",
    "            'labels': lit_types.Embeddings(),\n",
    "            'sense-labels': lit_types.Embeddings(),\n",
    "        }\n",
    "\n",
    "class LIT_Model(Model):\n",
    "    def __init__(self, path: str):\n",
    "        self._model = load_model(path)\n",
    "        \n",
    "    def input_spec(self):\n",
    "        return {\n",
    "            'input_ids': lit_types.Embeddings(),\n",
    "            'attention_mask': lit_types.Embeddings(),\n",
    "            'labels': lit_types.Embeddings(),\n",
    "            #'sense-labels': lit_types.Embeddings(),\n",
    "            #'sentence': lit_types.TextSegment(),\n",
    "        }\n",
    "    \n",
    "    def output_spec(self):\n",
    "        return {\n",
    "            'prediction': lit_types.CategoryLabel(),\n",
    "        }\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        inputs = list(inputs)\n",
    "        \n",
    "        if len(inputs) == 0:\n",
    "            return []\n",
    "        \n",
    "        pred = self._model.predict(inputs)\n",
    "        logits = pred.predictions\n",
    "        label_ids = pred.label_ids\n",
    "        \n",
    "        output = []\n",
    "        \n",
    "        n = logits.shape[0] // len(inputs)\n",
    "        for i in range(len(inputs)):\n",
    "            masked_labels = label_ids[0][i]\n",
    "            sense_labels = label_ids[1][i]\n",
    "            sense_labels[masked_labels == -100] = -100\n",
    "            lossable = (sense_labels != -100)\n",
    "            probas = logits[n*i:n*(i+1)]\n",
    "            prediction = np.argmax(probas, axis=-1)[lossable.flatten()]\n",
    "            if len(prediction) == 0:\n",
    "                prediction = [1]\n",
    "            token = self._model.tokenizer.decode(prediction[0])\n",
    "            output.append({'prediction': token})\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def predict_minibatch(self, inputs):\n",
    "        return self._model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWhbAZg57RpB"
   },
   "outputs": [],
   "source": [
    "# Create the LIT widget with the model and dataset to analyze.\n",
    "from lit_nlp import notebook\n",
    "\n",
    "#datasets = {'sst_dev': glue.SST2Data('validation')}\n",
    "lit_models = {'wsd': LIT_Model('./checkpoints-probing/roberta-probing+semcor/checkpoint-185900')}\n",
    "lit_datasets = {'wsd': LIT_Dataset('./dataset/roberta+senseval2.pickle')}\n",
    "\n",
    "widget = notebook.LitWidget(lit_models, lit_datasets, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GSfs1waBdLd"
   },
   "outputs": [],
   "source": [
    "# Render the widget\n",
    "widget.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('./dataset/roberta+senseval2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LIT in Notebooks",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
