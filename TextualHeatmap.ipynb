{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8GJbpoUmYdT"
   },
   "source": [
    "# Saliency Maps with HuggingFace and TextualHeatmap\n",
    "\n",
    "This notebook implements the saliency map as described in [Andreas Madsen's distill paper](https://distill.pub/2019/memorization-in-rnns/). However, it apply the method on BERT models rather than RNN models.\n",
    "\n",
    "The visualization therefore describes which words/sub-words were important for infering a masked word/sub-word.\n",
    "\n",
    "* TextualHeatmap: https://github.com/AndreasMadsen/python-textualheatmap\n",
    "* HuggingFace Transformers: https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpST5Dc0nNNt"
   },
   "source": [
    "First install TensorFlow, TextualHeatmap, and Transformers. In this notebook the TensorFlow implementations in `transfomers` are used, however this could also be applyed to the PyTorch implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tur-G6LngqL"
   },
   "source": [
    "As described in [Andreas Madsen's distill paper](https://distill.pub/2019/memorization-in-rnns/), the saliency map is computed by measuring the gradient magnitude of the output w.r.t. the input.\n",
    "\n",
    "$$\n",
    "\\mathrm{connectivity}(t, \\tilde{t}) = \\left|\\left| \\frac{\\partial y^{\\tilde{t}}_{k}}{\\partial x^t} \\right|\\right|_2\n",
    "$$\n",
    "\n",
    "Implementation wise this can be done quite easily with `tf.GradientTape`. However, because the gradient can not be take w.r.t. to an `int32` type, which is how the `token_ids` are encoded, an one-hot-encoding should be used instead. HuggingFace Transformers supports this via `inputs_embeds` which is the actual input-word-embedding, thus by computing $\\mathbf{x} \\mathbf{W}$ in the `tf.GradientTape` scope the gradient w.r.t. $\\mathbf{x}$ can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "IMyHY55SC24O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_textual_saliency(model, embedding_matrix, tokenizer, text):\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    vocab_size = embedding_matrix.num_embeddings\n",
    "\n",
    "    print(token_ids)\n",
    "\n",
    "    heatmap_data = []\n",
    "\n",
    "    for masked_token_index in range(len(token_ids)):\n",
    "        # print(f'processing token {masked_token_index + 1} / {len(token_ids)}')\n",
    "\n",
    "        if masked_token_index == 0:\n",
    "            heatmap_data.append({\n",
    "                'token': '[CLR]',\n",
    "                'meta': ['', '', ''],\n",
    "                'heat': [1] + [0] * (len(token_ids) - 1)\n",
    "            })\n",
    "        elif masked_token_index == len(token_ids) - 1:\n",
    "            heatmap_data.append({\n",
    "                'token': ' ',\n",
    "                'format': True\n",
    "            })\n",
    "            heatmap_data.append({\n",
    "                'token': '[SEP]',\n",
    "                'meta': ['', '', ''],\n",
    "                'heat': [0] * (len(token_ids) - 1) + [1]\n",
    "            })\n",
    "        else:\n",
    "            # Get the actual token\n",
    "            target_token = tokenizer.convert_ids_to_tokens(\n",
    "                token_ids[masked_token_index])\n",
    "\n",
    "            if target_token[0:2] == '##':\n",
    "                target_token = target_token[2:]\n",
    "            else:\n",
    "                heatmap_data.append({\n",
    "                    'token': ' ',\n",
    "                    'format': True\n",
    "                })\n",
    "\n",
    "            # integers are not differentable, so use a one-hot encoding\n",
    "            # of the intput\n",
    "            token_ids_tensor = torch.tensor(token_ids, dtype=torch.int64)\n",
    "            token_ids_tensor[masked_token_index] = tokenizer.mask_token_id\n",
    "            token_ids_tensor_one_hot = F.one_hot(token_ids_tensor, vocab_size).float()\n",
    "            token_ids_tensor_one_hot.requires_grad = True\n",
    "\n",
    "            # To select, the correct output witch is what the importance\n",
    "            # measure targets, create a masking tensor. tf.gather_nd could also\n",
    "            # be used, but this is easier.\n",
    "            output_mask = torch.zeros((1, len(token_ids), model.num_classes))\n",
    "            # todo match here with correct label / only mark correct one with 1?\n",
    "            output_mask[0, masked_token_index, :] = 1\n",
    "\n",
    "            # Compute gradient of the logits of the correct target, w.r.t. the\n",
    "            # input\n",
    "\n",
    "            inputs_embeds = torch.matmul(token_ids_tensor_one_hot, embeddings.weight)\n",
    "            dummy = torch.full_like(token_ids_tensor, -100)\n",
    "            output = model(**{\"inputs_embeds\": inputs_embeds.unsqueeze(dim=0), \"labels\": dummy, \"sense-labels\": dummy})\n",
    "            logits = output.logits\n",
    "\n",
    "            # todo fixme ? to be matched with original label\n",
    "            predict_mask_correct_token = torch.sum(logits * output_mask)\n",
    "\n",
    "            print(predict_mask_correct_token)\n",
    "\n",
    "            # Get the top-3 predictions\n",
    "            (_, top_3_indices) = torch.topk(logits[masked_token_index, :], 3)\n",
    "            # top_3_predicted_tokens = tokenizer.convert_ids_to_tokens(top_3_indices)\n",
    "\n",
    "            predict_mask_correct_token.backward()\n",
    "\n",
    "            # compute the connectivity\n",
    "            connectivity_non_normalized = torch.norm(token_ids_tensor_one_hot.grad, dim=1)\n",
    "            connectivity_tensor = (\n",
    "                connectivity_non_normalized /\n",
    "                torch.max(connectivity_non_normalized)\n",
    "            )\n",
    "\n",
    "            print(connectivity_tensor)\n",
    "            connectivity = connectivity_tensor[0].numpy().tolist()\n",
    "\n",
    "            # todo - zero grads!\n",
    "            # token_ids_tensor_one_hot.zero_()\n",
    "            # predict_mask_correct_token.zero_()\n",
    "            model.zero_grad()\n",
    "\n",
    "\n",
    "            heatmap_data.append({\n",
    "                'token': target_token,\n",
    "                'meta': top_3_indices.tolist(), # todo replace with sense-key names\n",
    "                'heat': connectivity\n",
    "            })\n",
    "\n",
    "    return heatmap_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CF-V7r0o-g7"
   },
   "source": [
    "With this implementation, it is now possible to compare different BERT-like models. In theory any model can be compared, as long as the tokenization is the same. In this case the BERT and DistillBERT models are very similar, which is what we would expect and want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model_name(hf_model: str):\n",
    "    if \"bert-wwm\" in hf_model:\n",
    "        model_name = \"bert-large-uncased-whole-word-masking\"\n",
    "    elif \"roberta\" in hf_model:\n",
    "        model_name = \"roberta-base\"\n",
    "    else:\n",
    "        assert \"deberta\" in hf_model\n",
    "        model_name = \"microsoft/deberta-base\"\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "KzS9z-lVeBTl",
    "outputId": "74ec05a4-cec7-407a-8729-8fcaaa2c1d7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.textual-heatmap .facet {\n  display: grid;\n  grid-template-columns: 1fr 60px;\n  grid-template-rows: 40px auto;\n  grid-template-areas:\n    \"meta-content .\"\n    \"token-content facet-title\";\n  margin-bottom: 0.5em;\n}\n\n.textual-heatmap .facet.hide-meta-content {\n  grid-template-rows: auto;\n  grid-template-areas:\n    \"token-content facet-title\";\n}\n\n.textual-heatmap .facet .token-content {\n  grid-area: token-content;\n\n  font-family: monospace;\n  padding: 0.5em;\n  box-sizing: border-box;\n  background: #365d8d;\n  color: white;\n  line-height: 1.4em;\n  border-radius: 0 0 0 5px;\n}\n\n.textual-heatmap .facet.hide-meta-content .token-content {\n    border-radius: 5px 0 0 5px;\n}\n\n.textual-heatmap .facet .token-content span {\n  border-bottom: 0.2em solid transparent;\n  white-space: pre-wrap;\n}\n\n.textual-heatmap .facet .token-content span.selected {\n  border-bottom-color: white;\n}\n\n.textual-heatmap .facet .meta-content {\n  grid-area: meta-content;\n  display: grid;\n  grid-template-columns: repeat(auto-fit, minmax(0px, 1fr));\n  grid-column-gap: 2px;\n  align-items: center;\n}\n\n.textual-heatmap .facet.hide-meta-content .meta-content {\n    display: none;\n}\n\n.textual-heatmap .facet .meta-content .meta-content-item {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  padding: 0 5px;\n  height: 39px;\n  background: #F3F3F3;\n  border: 1px solid #E0E0E0;\n  color: black;\n  border-bottom: none;\n  text-overflow: ellipsis;\n  overflow: hidden;\n}\n\n.textual-heatmap .facet .meta-content .meta-content-item:first-of-type {\n    border-radius: 5px 0 0 0;\n}\n\n.textual-heatmap .facet .meta-content .meta-content-item:last-of-type {\n    border-radius: 0 5px 0 0;\n}\n\n.textual-heatmap .facet .meta-content .meta-content-item:first-of-type:last-of-type {\n    border-radius: 5px 5px 0 0;\n}\n\n.textual-heatmap .facet .facet-title {\n  grid-area: facet-title;\n\n  display: flex;\n  max-width: 60px;\n  background: #EEEEEE;\n  justify-content: center;\n  color: #555555;\n  border-radius: 0 5px 5px 0;\n  border: 1px solid #E0E0E0;\n  border-left: none;\n}\n\n.textual-heatmap .facet .facet-title span {\n  align-self: center;\n  display: inline-block;\n  transform-origin: center center;\n  line-height: 1em;\n  text-align: center;\n}\n\n.textual-heatmap .facet.rotate-facet-title .facet-title span {\n  transform: translate(0, 0) rotate(90deg);\n}</style><script>;(function () {\n    'use strict';\n\n    function viridisSubset(ratio) {\n        const colormap = [\n            '#365d8d', '#355e8d', '#355f8d', '#34608d', '#34618d',\n            '#33628d', '#33638d', '#32648e', '#32658e', '#31668e',\n            '#31678e', '#31688e', '#30698e', '#306a8e', '#2f6b8e',\n            '#2f6c8e', '#2e6d8e', '#2e6e8e', '#2e6f8e', '#2d708e',\n            '#2d718e', '#2c718e', '#2c728e', '#2c738e', '#2b748e',\n            '#2b758e', '#2a768e', '#2a778e', '#2a788e', '#29798e',\n            '#297a8e', '#297b8e', '#287c8e', '#287d8e', '#277e8e',\n            '#277f8e', '#27808e', '#26818e', '#26828e', '#26828e',\n            '#25838e', '#25848e', '#25858e', '#24868e', '#24878e',\n            '#23888e', '#23898e', '#238a8d', '#228b8d', '#228c8d',\n            '#228d8d', '#218e8d', '#218f8d', '#21908d', '#21918c',\n            '#20928c', '#20928c', '#20938c', '#1f948c', '#1f958b',\n            '#1f968b', '#1f978b', '#1f988b', '#1f998a', '#1f9a8a',\n            '#1e9b8a', '#1e9c89', '#1e9d89', '#1f9e89', '#1f9f88',\n            '#1fa088', '#1fa188', '#1fa187', '#1fa287', '#20a386',\n            '#20a486', '#21a585', '#21a685', '#22a785', '#22a884',\n            '#23a983', '#24aa83', '#25ab82', '#25ac82', '#26ad81',\n            '#27ad81', '#28ae80', '#29af7f', '#2ab07f', '#2cb17e',\n            '#2db27d', '#2eb37c', '#2fb47c', '#31b57b', '#32b67a',\n            '#34b679', '#35b779'\n        ];\n        const n = colormap.length - 1;\n        return colormap[Math.max(0, Math.min(n, Math.floor(ratio * n)))];\n    }\n\n    class TextualHeatmap {\n        constructor(settings) {\n            this.container = document.getElementById(settings.id);\n            this.container.style.width = settings.width + 'px';\n            this.facets = settings.facetTitles\n                .map((facetName) => new TextualHeatmapFacet(settings, this.container, facetName));\n\n            for (let i = 0; i < this.facets.length; i++) {\n                this.facets[i].onmouseover = this.highlight.bind(this);\n            }\n        }\n\n        setData(data) {\n            for (let i = 0; i < this.facets.length; i++) {\n                this.facets[i].setData(data[i]);\n            }\n        }\n\n        highlight(index) {\n            for (let i = 0; i < this.facets.length; i++) {\n                this.facets[i].highlight(index);\n            }\n        }\n    }\n\n    class TextualHeatmapFacet {\n        constructor(settings, root, facetName) {\n            this.settings = settings;\n            this.highlightIndex = null;\n            this.nonFormatData = [];\n            this.heatIndexToNodeElement = [];\n            this.root = root;\n            this.onmouseover = null;\n\n            this.facet = document.createElement('div');\n            this.facet.classList.add('facet');\n            this.facet.classList.toggle('hide-meta-content', !settings.showMeta);\n            this.facet.classList.toggle('rotate-facet-title', settings.rotateFacetTitles);\n            this.root.appendChild(this.facet);\n\n            this.meta = document.createElement('div');\n            this.meta.classList.add('meta-content');\n            this.facet.appendChild(this.meta);\n\n            const item = document.createElement('div');\n            item.classList.add('meta-content-item');\n            this.meta.appendChild(item);\n\n            this.content = document.createElement('div');\n            this.content.classList.add('token-content');\n            this.facet.appendChild(this.content);\n\n            this.title = document.createElement('div');\n            this.title.classList.add('facet-title');\n            const titleSpan = document.createElement('span');\n            titleSpan.appendChild(document.createTextNode(facetName));\n            this.title.appendChild(titleSpan);\n            this.facet.appendChild(this.title);\n        }\n\n        setData(data) {\n            this.nonFormatData = [];\n            this.heatIndexToNodeElement = []\n\n            while (this.content.childNodes.length > 0) {\n                this.content.removeChild(this.content.firstChild);\n            }\n\n            for (let i = 0; i < data.length; i++) {\n                const tokenNode = document.createElement('span');\n                const heatIndex = this.heatIndexToNodeElement.length;\n                tokenNode.appendChild(document.createTextNode(data[i].token));\n                if (this.settings.interactive && !data[i].format) {\n                    tokenNode.addEventListener('mouseover', () => this.onmouseover(heatIndex), false);\n                    this.heatIndexToNodeElement.push(tokenNode)\n                    this.nonFormatData.push(data[i])\n                }\n                this.content.appendChild(tokenNode);\n            }\n\n            if (this.highlightIndex !== null) {\n                this.highlight(this.highlightIndex);\n            }\n        }\n\n        highlight(index) {\n            this.highlightIndex = index;\n\n            for (let i = 0; i < this.heatIndexToNodeElement.length; i++) {\n                this.heatIndexToNodeElement[i].style.backgroundColor = viridisSubset(this.nonFormatData[index].heat[i]);\n                this.heatIndexToNodeElement[i].classList.toggle('selected', i === index);\n            }\n\n            if (this.settings.showMeta) {\n                while (this.meta.childNodes.length > 0) {\n                    this.meta.removeChild(this.meta.firstChild);\n                }\n\n                for (let i = 0; i < this.nonFormatData[index].meta.length; i++) {\n                    const item = document.createElement('div');\n                    item.classList.add('meta-content-item');\n                    item.appendChild(document.createTextNode(this.nonFormatData[index].meta[i]));\n                    this.meta.appendChild(item);\n                }\n\n                if (this.nonFormatData[index].meta.length === 0) {\n                    const item = document.createElement('div');\n                    item.classList.add('meta-content-item');\n                    this.meta.appendChild(item);\n                }\n            }\n        }\n    }\n\n    window.setupTextualHeatmap = function (settings) {\n        document.getElementById(settings.id).instance = new TextualHeatmap(settings);\n    };\n\n    window.setDataTextualHeatmap = function (settings, data) {\n        document.getElementById(settings.id).instance.setData(data);\n    };\n\n    window.highlightTextualHeatmap = function (settings, index) {\n        document.getElementById(settings.id).instance.highlight(index);\n    };\n})();</script><div id=\"495d038c-5390-4ff9-9756-8be79e9afcd8\" class=\"textual-heatmap\"></div><script>  window.setupTextualHeatmap({\"id\": \"495d038c-5390-4ff9-9756-8be79e9afcd8\", \"width\": 600, \"showMeta\": true, \"facetTitles\": [\"Roberta\"], \"rotateFacetTitles\": false, \"interactive\": true});</script>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "void(0);"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "void(0);"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 46796, 5, 4828, 892, 9, 33055, 16, 41, 505, 233, 9, 1265, 31, 10, 664, 1046, 149, 3319, 2239, 600, 5, 1492, 5850, 11, 1304, 32, 45, 10, 33055, 11, 5, 1472, 144, 38954, 1952, 304, 2]\n",
      "tensor(-90103.6953, grad_fn=<SumBackward0>)\n",
      "tensor([0.2899, 0.6737, 0.4252, 0.6075, 0.4417, 0.2493, 0.4475, 0.3164, 0.1965,\n",
      "        0.4759, 0.2452, 0.1722, 0.3359, 0.2107, 0.1845, 0.2607, 0.2184, 0.2045,\n",
      "        0.2501, 0.2939, 1.0000, 0.3461, 0.2916, 0.2617, 0.1471, 0.2924, 0.1913,\n",
      "        0.2512, 0.1861, 0.4380, 0.1778, 0.2066, 0.3312, 0.2248, 0.4726, 0.2581,\n",
      "        0.2823, 0.3032])\n",
      "tensor(-72087.4453, grad_fn=<SumBackward0>)\n",
      "tensor([0.1510, 1.0000, 0.2902, 0.4173, 0.2937, 0.0970, 0.1972, 0.1018, 0.0912,\n",
      "        0.1221, 0.0967, 0.0738, 0.1285, 0.0718, 0.0566, 0.0777, 0.0896, 0.0787,\n",
      "        0.0761, 0.0816, 0.1282, 0.0699, 0.0890, 0.0698, 0.0478, 0.0930, 0.0586,\n",
      "        0.0596, 0.0682, 0.0964, 0.0619, 0.0659, 0.0819, 0.0666, 0.1166, 0.0755,\n",
      "        0.2576, 0.4283])\n",
      "tensor(-96877.5781, grad_fn=<SumBackward0>)\n",
      "tensor([0.1381, 0.3988, 0.3853, 0.3504, 1.0000, 0.2421, 0.5241, 0.1510, 0.1114,\n",
      "        0.2620, 0.1479, 0.1130, 0.2743, 0.1175, 0.0940, 0.1672, 0.1396, 0.1141,\n",
      "        0.2423, 0.2079, 0.2077, 0.1170, 0.1918, 0.1698, 0.0923, 0.2600, 0.0774,\n",
      "        0.0935, 0.0760, 0.2332, 0.0785, 0.0766, 0.1220, 0.0913, 0.3324, 0.1127,\n",
      "        0.1075, 0.1888])\n",
      "tensor(-87423.2500, grad_fn=<SumBackward0>)\n",
      "tensor([0.1264, 0.1501, 0.1549, 1.0000, 0.2116, 0.1363, 0.4831, 0.1108, 0.0671,\n",
      "        0.1394, 0.1090, 0.0670, 0.1830, 0.0754, 0.0698, 0.0962, 0.0878, 0.0824,\n",
      "        0.1082, 0.1092, 0.1104, 0.0716, 0.3342, 0.1513, 0.0562, 0.1024, 0.0667,\n",
      "        0.0699, 0.0854, 0.3080, 0.0720, 0.0623, 0.0965, 0.0615, 0.1816, 0.0741,\n",
      "        0.1079, 0.0985])\n",
      "tensor(-82432.1094, grad_fn=<SumBackward0>)\n",
      "tensor([0.1967, 0.5997, 0.2973, 0.5571, 0.6429, 0.3159, 1.0000, 0.1593, 0.1181,\n",
      "        0.2229, 0.1135, 0.0874, 0.2409, 0.0905, 0.0895, 0.1457, 0.1212, 0.0986,\n",
      "        0.1684, 0.1956, 0.1539, 0.0970, 0.2870, 0.1885, 0.0806, 0.2473, 0.0783,\n",
      "        0.0944, 0.1021, 0.3952, 0.0954, 0.0953, 0.1595, 0.1126, 0.4127, 0.1096,\n",
      "        0.2288, 0.3022])\n",
      "tensor(-73033.1875, grad_fn=<SumBackward0>)\n",
      "tensor([0.1768, 0.1919, 0.1503, 0.4324, 0.4316, 0.1547, 0.1958, 0.0865, 0.0712,\n",
      "        0.1156, 0.0833, 0.0783, 0.1737, 0.0745, 0.0705, 0.0906, 0.0917, 0.0741,\n",
      "        0.1128, 0.1732, 0.1381, 0.1228, 0.7778, 0.2237, 0.0888, 0.2314, 0.0791,\n",
      "        0.0978, 0.1158, 0.6333, 0.1003, 0.0938, 0.1491, 0.1780, 1.0000, 0.2436,\n",
      "        0.1539, 0.1734])\n",
      "tensor(-91189.5469, grad_fn=<SumBackward0>)\n",
      "tensor([0.3162, 0.6432, 0.4985, 0.7357, 0.7110, 0.2808, 0.5535, 0.9055, 0.4419,\n",
      "        1.0000, 0.5962, 0.3509, 0.9083, 0.2876, 0.2534, 0.3173, 0.3635, 0.2773,\n",
      "        0.4055, 0.3082, 0.7659, 0.2420, 0.3098, 0.2716, 0.1414, 0.2917, 0.1955,\n",
      "        0.2397, 0.1883, 0.4126, 0.1485, 0.1690, 0.2979, 0.1804, 0.4490, 0.2391,\n",
      "        0.2587, 0.3550])\n",
      "tensor(-90519.3906, grad_fn=<SumBackward0>)\n",
      "tensor([0.2072, 0.3483, 0.2527, 0.3778, 0.4404, 0.1682, 0.3238, 0.3796, 0.5050,\n",
      "        1.0000, 0.4952, 0.2684, 0.7286, 0.3528, 0.1765, 0.2676, 0.3943, 0.3404,\n",
      "        0.3402, 0.3822, 0.6275, 0.2535, 0.2206, 0.2249, 0.1149, 0.2651, 0.1384,\n",
      "        0.2088, 0.1448, 0.2703, 0.1132, 0.1265, 0.2715, 0.1512, 0.2567, 0.1184,\n",
      "        0.2457, 0.2623])\n",
      "tensor(-85045.6016, grad_fn=<SumBackward0>)\n",
      "tensor([0.3259, 0.3956, 0.3707, 1.0000, 0.6885, 0.2566, 0.6697, 0.3942, 0.4294,\n",
      "        0.6420, 0.8704, 0.4321, 0.9642, 0.3234, 0.2519, 0.4393, 0.4330, 0.3245,\n",
      "        0.6418, 0.3868, 0.7775, 0.3016, 0.3531, 0.2962, 0.1764, 0.4129, 0.2021,\n",
      "        0.3169, 0.1933, 0.4225, 0.1871, 0.1852, 0.3988, 0.2367, 0.4469, 0.2002,\n",
      "        0.2785, 0.3476])\n",
      "tensor(-90452.7344, grad_fn=<SumBackward0>)\n",
      "tensor([0.2948, 0.2832, 0.3996, 0.7637, 0.7561, 0.2240, 0.5070, 0.3096, 0.3155,\n",
      "        1.0000, 0.5329, 0.3421, 0.8505, 0.3535, 0.1986, 0.3343, 0.3847, 0.5009,\n",
      "        0.4622, 0.4028, 0.3712, 0.1916, 0.3441, 0.2619, 0.1422, 0.2893, 0.1493,\n",
      "        0.1808, 0.1381, 0.3194, 0.1550, 0.1724, 0.3144, 0.1789, 0.4112, 0.2376,\n",
      "        0.2253, 0.2649])\n",
      "tensor(-76815.1641, grad_fn=<SumBackward0>)\n",
      "tensor([0.1669, 0.2056, 0.1551, 0.3266, 0.2518, 0.0997, 0.2245, 0.1588, 0.1614,\n",
      "        0.4156, 0.6336, 0.4055, 1.0000, 0.2469, 0.1349, 0.2143, 0.2360, 0.2190,\n",
      "        0.2741, 0.2239, 0.2571, 0.1227, 0.1674, 0.1625, 0.0904, 0.2804, 0.0765,\n",
      "        0.0859, 0.0660, 0.1638, 0.0732, 0.0792, 0.1241, 0.0944, 0.2422, 0.1105,\n",
      "        0.1163, 0.1295])\n",
      "tensor(-79764.0312, grad_fn=<SumBackward0>)\n",
      "tensor([0.2819, 0.3186, 0.2676, 0.5075, 0.4702, 0.1845, 0.6975, 0.2421, 0.2820,\n",
      "        0.5872, 0.5499, 0.4303, 0.4886, 0.4834, 0.2886, 0.4254, 0.4862, 0.5910,\n",
      "        0.6121, 0.6688, 0.3979, 0.1767, 0.4152, 0.2382, 0.1357, 0.3789, 0.1241,\n",
      "        0.1245, 0.1175, 0.4490, 0.1237, 0.1236, 0.1887, 0.2029, 1.0000, 0.2879,\n",
      "        0.1935, 0.2117])\n",
      "tensor(-74804.8359, grad_fn=<SumBackward0>)\n",
      "tensor([0.1982, 0.1107, 0.1162, 0.1605, 0.2198, 0.1034, 0.1763, 0.1065, 0.0949,\n",
      "        0.1628, 0.1640, 0.1538, 0.4734, 0.3111, 0.2578, 0.6164, 0.5217, 0.7895,\n",
      "        1.0000, 0.9467, 0.2438, 0.1251, 0.1828, 0.2123, 0.1009, 0.2425, 0.0749,\n",
      "        0.0975, 0.1015, 0.1282, 0.0869, 0.0806, 0.0924, 0.0919, 0.1768, 0.0772,\n",
      "        0.0777, 0.0991])\n",
      "tensor(-91036.6016, grad_fn=<SumBackward0>)\n",
      "tensor([0.1593, 0.0978, 0.1217, 0.1616, 0.1584, 0.0941, 0.1758, 0.0995, 0.0818,\n",
      "        0.1772, 0.1311, 0.1023, 0.2737, 0.3970, 0.4129, 1.0000, 0.8894, 0.4500,\n",
      "        0.4325, 0.3265, 0.2092, 0.1142, 0.1747, 0.1392, 0.0870, 0.1987, 0.0753,\n",
      "        0.0977, 0.0926, 0.1644, 0.0807, 0.0855, 0.1181, 0.1054, 0.1600, 0.0905,\n",
      "        0.1175, 0.1706])\n",
      "tensor(-102326.6016, grad_fn=<SumBackward0>)\n",
      "tensor([0.1928, 0.1548, 0.1117, 0.2718, 0.1856, 0.1033, 0.2120, 0.1129, 0.0919,\n",
      "        0.2108, 0.1527, 0.1340, 0.4013, 0.3709, 0.2971, 0.4297, 1.0000, 0.3730,\n",
      "        0.7776, 0.6132, 0.2357, 0.1312, 0.2497, 0.2150, 0.1237, 0.3242, 0.0851,\n",
      "        0.0953, 0.0946, 0.1804, 0.1113, 0.0842, 0.1283, 0.0995, 0.2358, 0.0928,\n",
      "        0.1155, 0.1872])\n",
      "tensor(-98208.4844, grad_fn=<SumBackward0>)\n",
      "tensor([0.1745, 0.1409, 0.1201, 0.2600, 0.2092, 0.1058, 0.2546, 0.1239, 0.1038,\n",
      "        0.1891, 0.1638, 0.1511, 0.3989, 0.4179, 0.2717, 1.0000, 0.4846, 0.4068,\n",
      "        0.5900, 0.4859, 0.2686, 0.1340, 0.2524, 0.2017, 0.1013, 0.2678, 0.0933,\n",
      "        0.1181, 0.1022, 0.2123, 0.0908, 0.0922, 0.1442, 0.1143, 0.3046, 0.1244,\n",
      "        0.1099, 0.1230])\n",
      "tensor(-64638.5625, grad_fn=<SumBackward0>)\n",
      "tensor([0.2135, 0.2815, 0.1816, 0.2176, 0.2455, 0.1195, 0.1974, 0.1654, 0.1381,\n",
      "        0.2600, 0.1996, 0.1712, 0.4535, 0.5488, 0.3084, 0.5390, 0.6313, 0.6078,\n",
      "        1.0000, 0.9141, 0.5359, 0.2339, 0.2821, 0.2978, 0.1181, 0.1918, 0.1309,\n",
      "        0.1765, 0.1102, 0.1858, 0.1176, 0.1368, 0.2152, 0.1383, 0.2477, 0.1242,\n",
      "        0.1681, 0.1716])\n",
      "tensor(-83457.4688, grad_fn=<SumBackward0>)\n",
      "tensor([0.1308, 0.1210, 0.1061, 0.2375, 0.1715, 0.0710, 0.2489, 0.0857, 0.0697,\n",
      "        0.1378, 0.0969, 0.0807, 0.3015, 0.1293, 0.0934, 0.2155, 0.1914, 0.2751,\n",
      "        0.2602, 1.0000, 0.4628, 0.1587, 0.2615, 0.2034, 0.0870, 0.2325, 0.0836,\n",
      "        0.1082, 0.0700, 0.1813, 0.0776, 0.0842, 0.1145, 0.1020, 0.2857, 0.0986,\n",
      "        0.1188, 0.1391])\n",
      "tensor(-75166.5391, grad_fn=<SumBackward0>)\n",
      "tensor([0.1393, 0.1532, 0.1141, 0.2220, 0.2006, 0.0858, 0.2654, 0.1044, 0.0727,\n",
      "        0.1312, 0.0972, 0.0842, 0.2731, 0.1620, 0.0983, 0.1795, 0.2266, 0.2520,\n",
      "        1.0000, 0.3156, 0.4472, 0.1162, 0.2943, 0.1766, 0.0823, 0.2235, 0.0893,\n",
      "        0.0942, 0.0869, 0.1859, 0.0773, 0.0739, 0.1160, 0.1064, 0.3910, 0.1452,\n",
      "        0.1252, 0.1362])\n",
      "tensor(-72470.4219, grad_fn=<SumBackward0>)\n",
      "tensor([0.2289, 1.0000, 0.3407, 0.2859, 0.3041, 0.1101, 0.2538, 0.1603, 0.1382,\n",
      "        0.3400, 0.1593, 0.1004, 0.2231, 0.1254, 0.0959, 0.1222, 0.1339, 0.1470,\n",
      "        0.2352, 0.3432, 0.4002, 0.2005, 0.2351, 0.1613, 0.0905, 0.1674, 0.1205,\n",
      "        0.1417, 0.1267, 0.2360, 0.1338, 0.1405, 0.1828, 0.1381, 0.2243, 0.1065,\n",
      "        0.2778, 0.3996])\n",
      "tensor(-84511.7266, grad_fn=<SumBackward0>)\n",
      "tensor([0.1099, 0.0760, 0.0659, 0.1637, 0.1239, 0.0675, 0.4142, 0.0760, 0.0534,\n",
      "        0.0837, 0.0612, 0.0634, 0.1372, 0.0479, 0.0551, 0.0623, 0.0544, 0.0514,\n",
      "        0.1081, 0.1153, 0.2172, 0.2027, 1.0000, 0.2327, 0.1004, 0.3280, 0.1025,\n",
      "        0.0950, 0.1345, 0.3378, 0.0932, 0.0922, 0.1292, 0.0894, 0.2763, 0.0867,\n",
      "        0.1255, 0.1191])\n",
      "tensor(-80133.2812, grad_fn=<SumBackward0>)\n",
      "tensor([0.2476, 0.1834, 0.1744, 0.3170, 0.3143, 0.1754, 0.9305, 0.1604, 0.1173,\n",
      "        0.1637, 0.1392, 0.1524, 0.3590, 0.1278, 0.1215, 0.1657, 0.1612, 0.1575,\n",
      "        0.2779, 0.3067, 0.3341, 0.3269, 0.4248, 0.8649, 0.2265, 0.4887, 0.3034,\n",
      "        0.2360, 0.3502, 1.0000, 0.2584, 0.2681, 0.3943, 0.2540, 0.7946, 0.2288,\n",
      "        0.3320, 0.2908])\n",
      "tensor(-78293.9062, grad_fn=<SumBackward0>)\n",
      "tensor([0.1125, 0.0854, 0.0787, 0.2272, 0.2348, 0.0863, 0.3263, 0.0825, 0.0774,\n",
      "        0.1005, 0.0859, 0.0628, 0.2035, 0.0710, 0.0598, 0.1117, 0.1034, 0.0785,\n",
      "        0.1363, 0.1807, 0.2363, 0.1920, 1.0000, 0.2405, 0.2192, 0.4998, 0.1349,\n",
      "        0.1193, 0.1175, 0.3125, 0.1201, 0.0964, 0.1582, 0.0968, 0.2190, 0.1029,\n",
      "        0.1171, 0.1384])\n",
      "tensor(-82161.5938, grad_fn=<SumBackward0>)\n",
      "tensor([0.2044, 0.1593, 0.1501, 0.3192, 0.2520, 0.1147, 0.3018, 0.1170, 0.1124,\n",
      "        0.1800, 0.1420, 0.1222, 0.4059, 0.1238, 0.1189, 0.2198, 0.2005, 0.1517,\n",
      "        0.2827, 0.2855, 0.3812, 0.2838, 0.5740, 0.8837, 0.3901, 1.0000, 0.2075,\n",
      "        0.1797, 0.1812, 0.3485, 0.1588, 0.1864, 0.2023, 0.1624, 0.2902, 0.1657,\n",
      "        0.1918, 0.1882])\n",
      "tensor(-80241.0391, grad_fn=<SumBackward0>)\n",
      "tensor([0.2511, 0.2806, 0.1797, 0.6873, 0.2988, 0.1373, 0.7677, 0.1692, 0.1318,\n",
      "        0.2251, 0.1909, 0.1451, 0.5056, 0.1354, 0.1343, 0.2122, 0.2021, 0.1855,\n",
      "        0.2797, 0.2982, 0.4387, 0.3077, 1.0000, 0.8631, 0.3948, 0.4159, 0.2635,\n",
      "        0.2270, 0.2252, 0.5633, 0.1805, 0.1932, 0.2573, 0.2102, 0.7876, 0.2684,\n",
      "        0.2222, 0.2374])\n",
      "tensor(-92761.4062, grad_fn=<SumBackward0>)\n",
      "tensor([0.2324, 0.2362, 0.1434, 0.2199, 0.2277, 0.1421, 0.5996, 0.1681, 0.1397,\n",
      "        0.2272, 0.1261, 0.1206, 0.2468, 0.1743, 0.1193, 0.1600, 0.1349, 0.1514,\n",
      "        0.2057, 0.2387, 0.8400, 0.3324, 0.7945, 0.3276, 0.1635, 0.3427, 0.5539,\n",
      "        0.5982, 0.4393, 1.0000, 0.3514, 0.3185, 0.7316, 0.2790, 0.3426, 0.1794,\n",
      "        0.3574, 0.2595])\n",
      "tensor(-90410.6641, grad_fn=<SumBackward0>)\n",
      "tensor([0.2214, 0.3079, 0.1462, 0.2507, 0.2521, 0.1272, 0.6050, 0.1517, 0.1274,\n",
      "        0.2564, 0.1696, 0.1031, 0.1779, 0.1328, 0.1148, 0.1677, 0.1421, 0.1876,\n",
      "        0.1990, 0.1932, 0.6871, 0.2547, 1.0000, 0.3036, 0.1602, 0.2344, 0.3839,\n",
      "        0.4498, 0.4420, 0.7387, 0.3324, 0.3453, 0.7489, 0.3266, 0.4157, 0.2040,\n",
      "        0.4010, 0.2530])\n",
      "tensor(-74574.4922, grad_fn=<SumBackward0>)\n",
      "tensor([0.1464, 0.1428, 0.0962, 0.2944, 0.2176, 0.1042, 0.4779, 0.1015, 0.0832,\n",
      "        0.1020, 0.0755, 0.0630, 0.1627, 0.0684, 0.0647, 0.0855, 0.0772, 0.0786,\n",
      "        0.1178, 0.1573, 0.1930, 0.1687, 0.8024, 0.2553, 0.0980, 0.2280, 0.1710,\n",
      "        0.1801, 0.2651, 1.0000, 0.1889, 0.2082, 0.3862, 0.1659, 0.3511, 0.1607,\n",
      "        0.2678, 0.1793])\n",
      "tensor(-67254.4219, grad_fn=<SumBackward0>)\n",
      "tensor([0.1586, 0.1767, 0.1230, 0.2693, 0.2273, 0.1104, 0.5999, 0.1215, 0.1255,\n",
      "        0.1752, 0.1063, 0.0856, 0.1781, 0.0723, 0.0806, 0.1373, 0.1038, 0.0852,\n",
      "        0.1475, 0.1385, 0.2501, 0.2438, 1.0000, 0.3257, 0.1315, 0.2771, 0.2368,\n",
      "        0.2152, 0.3918, 0.4548, 0.2713, 0.2952, 0.6185, 0.2520, 0.4070, 0.1592,\n",
      "        0.4141, 0.2345])\n",
      "tensor(-83007.4609, grad_fn=<SumBackward0>)\n",
      "tensor([0.1248, 0.0955, 0.0668, 0.1285, 0.0937, 0.0744, 0.2289, 0.0849, 0.0830,\n",
      "        0.0989, 0.0648, 0.0726, 0.1011, 0.0720, 0.0581, 0.0593, 0.0584, 0.0810,\n",
      "        0.0777, 0.1156, 0.1788, 0.1000, 0.2119, 0.1469, 0.0873, 0.1084, 0.1043,\n",
      "        0.1227, 0.1685, 0.5409, 0.3209, 0.3368, 1.0000, 0.2336, 0.2301, 0.1481,\n",
      "        0.2685, 0.1529])\n",
      "tensor(-82082., grad_fn=<SumBackward0>)\n",
      "tensor([0.1529, 0.1860, 0.0946, 0.1601, 0.1244, 0.0786, 0.2424, 0.0835, 0.0802,\n",
      "        0.1537, 0.0912, 0.0731, 0.1172, 0.0748, 0.0683, 0.0991, 0.0830, 0.0922,\n",
      "        0.1271, 0.1005, 0.2644, 0.1432, 0.1849, 0.1635, 0.0979, 0.2038, 0.1715,\n",
      "        0.2148, 0.2532, 0.4902, 0.4651, 0.4694, 1.0000, 0.3427, 0.2498, 0.1989,\n",
      "        0.2727, 0.2069])\n",
      "tensor(-84555.4219, grad_fn=<SumBackward0>)\n",
      "tensor([0.2110, 0.1554, 0.1435, 0.2263, 0.1696, 0.1134, 0.4576, 0.1246, 0.1012,\n",
      "        0.1549, 0.1057, 0.1012, 0.1960, 0.0927, 0.0870, 0.1392, 0.1194, 0.1159,\n",
      "        0.1365, 0.1447, 0.2696, 0.1824, 0.4270, 0.2146, 0.1583, 0.2796, 0.2436,\n",
      "        0.2697, 0.3614, 1.0000, 0.4107, 0.4405, 0.4967, 0.4348, 0.5606, 0.3517,\n",
      "        0.6983, 0.4391])\n",
      "tensor(-88327.3750, grad_fn=<SumBackward0>)\n",
      "tensor([0.1408, 0.1256, 0.0728, 0.1669, 0.1551, 0.0677, 0.2917, 0.0753, 0.0629,\n",
      "        0.1063, 0.0726, 0.0624, 0.1328, 0.0594, 0.0645, 0.0732, 0.0815, 0.0655,\n",
      "        0.1217, 0.1360, 0.1511, 0.0976, 0.3102, 0.1636, 0.0860, 0.2222, 0.1095,\n",
      "        0.0972, 0.1145, 0.3393, 0.1633, 0.1646, 0.3899, 0.2918, 1.0000, 0.3588,\n",
      "        0.3264, 0.1729])\n",
      "tensor(-76020.3984, grad_fn=<SumBackward0>)\n",
      "tensor([0.1644, 0.1476, 0.0980, 0.3162, 0.2510, 0.1333, 0.8411, 0.1214, 0.1035,\n",
      "        0.1410, 0.0979, 0.0944, 0.2013, 0.0745, 0.0850, 0.1131, 0.0916, 0.0835,\n",
      "        0.1168, 0.2011, 0.1630, 0.1230, 0.6112, 0.2458, 0.1071, 0.3477, 0.1140,\n",
      "        0.0997, 0.1149, 0.6234, 0.1312, 0.1365, 0.2631, 0.2115, 0.3680, 1.0000,\n",
      "        0.2770, 0.1912])\n",
      "tensor(-74740.1484, grad_fn=<SumBackward0>)\n",
      "tensor([0.0537, 0.0572, 0.0261, 0.0515, 0.0435, 0.0229, 0.0790, 0.0247, 0.0207,\n",
      "        0.0290, 0.0203, 0.0253, 0.0421, 0.0183, 0.0216, 0.0272, 0.0255, 0.0216,\n",
      "        0.0376, 0.0416, 0.0387, 0.0284, 0.0501, 0.0356, 0.0272, 0.0525, 0.0302,\n",
      "        0.0305, 0.0325, 0.0952, 0.0329, 0.0408, 0.0745, 0.0969, 1.0000, 0.1385,\n",
      "        0.1263, 0.1025])\n",
      "tensor(-83640.7500, grad_fn=<SumBackward0>)\n",
      "tensor([0.3195, 0.4766, 0.2618, 0.3289, 0.2930, 0.1590, 0.2851, 0.2045, 0.1651,\n",
      "        0.2747, 0.1787, 0.1336, 0.2567, 0.1287, 0.1318, 0.1718, 0.1617, 0.1685,\n",
      "        0.2446, 0.2567, 0.4697, 0.2429, 0.3175, 0.2947, 0.1798, 0.2367, 0.2361,\n",
      "        0.2337, 0.2233, 0.4689, 0.2933, 0.4278, 0.8126, 0.7814, 0.6538, 0.4952,\n",
      "        1.0000, 0.9434])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from textualheatmap import TextualHeatmap\n",
    "import datagen\n",
    "from datagen.dataset import SemCorDataSet\n",
    "import datasets\n",
    "from modelling.model import SynsetClassificationModel\n",
    "\n",
    "text = (\"context the formal study of grammar is an important part of education\"\n",
    "        \" from a young age through advanced learning though the rules taught\"\n",
    "        \" in schools are not a grammar in the sense most linguists use\")\n",
    "\n",
    "model_name = \"out/checkpoints/roberta-probing+semcor/checkpoint-185900\"\n",
    "base_model_name = construct_model_name('roberta')\n",
    "config = AutoConfig.from_pretrained(model_name, local_files_only=True)\n",
    "cl_model = SynsetClassificationModel.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    local_files_only=True,\n",
    "    model_name=base_model_name,\n",
    "    num_classes=2584,\n",
    "    freeze_lm=False,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "embeddings = cl_model.mlmodel.embeddings.word_embeddings\n",
    "\n",
    "heatmap = TextualHeatmap(facet_titles = ['Roberta'], show_meta=True)\n",
    "heatmap.set_data([\n",
    "    compute_textual_saliency(cl_model, embeddings, tokenizer, text),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdlWmkslpUEG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I hope you found this useful, if so please consider sharing/retweeting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TextualHeatmap.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}